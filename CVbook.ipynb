{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "spare-illinois",
   "metadata": {},
   "source": [
    "# Bulk analysis of constant velocity experiments\n",
    "In this notebook we take a large number of constant velocity traces and inspect each of them for:\n",
    "- Unfolding events\n",
    "- Experimental errors\n",
    "  - Multiple tethers\n",
    "  - Bead loss\n",
    "\n",
    "After which we determine the contour lengths of unfolded domains as well as their unfolding forces, by fitting each curve with Odijk and Marko-Siggia models (for DNA handles and unfolded protein respectively).\n",
    "\n",
    "This is very much a work in progress and there are some things to keep in mind (also a todo/fix list):\n",
    "- For now, fdcurves have to include both the pulling and relaxation parts, even if we only care about the pull.\n",
    "- The error-finding capabilities are not yet proven to be effective.\n",
    "- There is no error-handling and limited sanity-checking: the notebook can fail silently and in unexpected ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "embedded-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lumicks.pylake as lk\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from curve import Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-secretary",
   "metadata": {},
   "source": [
    "## Defining models\n",
    "\n",
    "Here we define our models. As long as the below estimates correspond, you should be able to change these. The name strings (```'handles'``` and ```'protein'```) feature as part of parameter names and these should stay consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "injured-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these dont have to be build functions anymore, can just be model objects\n",
    "def build_handles_model():\n",
    "    return lk.inverted_odijk('handles') + lk.force_offset('handles')\n",
    "def build_composite_model():\n",
    "    comp_wrt_f = lk.odijk('handles') + lk.inverted_marko_siggia_simplified('protein')\n",
    "    return comp_wrt_f.invert(interpolate = True, \n",
    "                             independent_min = 0,\n",
    "                             independent_max = 90) + lk.force_offset('handles')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-bosnia",
   "metadata": {},
   "source": [
    "## Providing estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-reserve",
   "metadata": {},
   "source": [
    "Here we provide some initial guesses for the model parameters in a dictionary format.\n",
    "\n",
    "For each parameter, the possible entries are ```'value'```, ```'upper_bound'```, ```'lower_bound'``` and ```'fixed'```, where the latter fixes the parameter value to that in the 'value' field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "european-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_config import bp2cl\n",
    "\n",
    "handle_estimates = \\\n",
    "    {'handles/Lp':  # DNA handle persistence length (nm)\n",
    "      {'value': 15,  # initial estimate\n",
    "       'upper_bound': 100,  # very wide bounds?\n",
    "       'lower_bound': 0.0},\n",
    "     'handles/Lc':  # contour length (um)\n",
    "      {'value': 0.3},#bp2cl(1040)},  # bp2cl generates a contour length from a number of basepairs.\n",
    "     'handles/St':  # stretch modulus (pN)\n",
    "      {'value': 300,\n",
    "       'lower_bound': 250},\n",
    "     'handles/f_offset':  # force offset (pN)\n",
    "      {'value': 0,\n",
    "       'upper_bound': 6,\n",
    "       'lower_bound': -6,\n",
    "       'fixed': True}  # if True, the parameter is fixed to the current 'value' field.\n",
    "    }                 # here we use this entry to turn off the force offset included in the model\n",
    "protein_estimates = \\\n",
    "     {'protein/Lp':  # unfolded protein persistence length (nm)\n",
    "      {'value': 0.7,\n",
    "      'upper_bound': 1.0,\n",
    "      'lower_bound': 0.6,\n",
    "      'fixed': False},\n",
    "     'protein/Lc':  # contour length (um)\n",
    "      {'value': 0.001,\n",
    "       'fixed': False}\n",
    "     }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-redhead",
   "metadata": {},
   "source": [
    "## Providing data\n",
    "To enter your data, replace the 'filename' fields with paths to your .h5 files, and include the names for your corresponding fdcurves in the list after 'curve_ids'. Each entry in the datasets list should look something like this:\n",
    "```\n",
    "{'filename' : 'PATH/TO/FILE.h5', 'curve_ids': ['CURVE1', 'CURVE2', 'CURVE3']}\n",
    "```\n",
    "Each fdcurve will be assigned a Curve object, which holds the force and distance data and has functions we will use to analyze the it.\n",
    "\n",
    "We can use the ```PRUNE_ZEROS``` option below if we suspect datapoints with a distance measurement of zero or less. ```PREPEND_FILENAME``` determines if we prepend the origin filename to each curve id in order to enforce unique identifiers. Overlapping identifiers will result in only one of the affected fdcurves being analyzed.\n",
    "\n",
    "From here the notebook should be able to run without user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "recreational-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should add the option to include all fdcurves found in a file\n",
    "datasets = [{'filename': '/home/kris/proj/.data/tweez/adk5_curve1.h5', 'curve_ids': ['adk5_curve1']},\n",
    "            {'filename': '/home/kris/proj/.data/tweez/adk5_curve2.h5', 'curve_ids': ['adk5_curve2']},\n",
    "            {'filename': '/home/kris/proj/.data/tweez/adk5_curve3.h5', 'curve_ids': ['adk5_curve3']}]\n",
    "PRUNE_ZEROS = True\n",
    "PREPEND_FILENAME = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-orchestra",
   "metadata": {},
   "source": [
    "datasets = [{'filename': '20210302-190729 Marker 4_TrmD.h5'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "million-peoples",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdcurves = {}\n",
    "for dataset in datasets:\n",
    "    data = lk.File(dataset['filename'])\n",
    "    #print(data.fdcurves)\n",
    "    for curve_id in dataset['curve_ids']:\n",
    "        original = data.fdcurves[curve_id]\n",
    "        fdata = data.fdcurves[curve_id].f.data\n",
    "        ddata = data.fdcurves[curve_id].d.data\n",
    "        if PRUNE_ZEROS:\n",
    "            fdata = fdata[ddata > 0]\n",
    "            ddata = ddata[ddata > 0]\n",
    "        if PREPEND_FILENAME:\n",
    "            curve_id = dataset['filename'] + ':' + curve_id\n",
    "        fdcurves[curve_id] = Curve(curve_id, ddata, fdata)\n",
    "        # we don't need the id in two different places"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-advancement",
   "metadata": {},
   "source": [
    "If for some reason you want to add simulated data, run this cell with ```DEBUG = True```. The simulations are a bit off. The arguments ```generate_fd``` takes are:\n",
    "- the distance at which the first unfolding event happens\n",
    "- a list containing unfolding domain contour lengths\n",
    "- dna handle and protein parameter estimates in the same format as above\n",
    "\n",
    "If you have any other sources of data, you can similarly include them by adding more Curve objects to the fdcurves dictionary. This would look like the following:\n",
    "```\n",
    "fdcurves[ID] = Curve(ID, DISTANCE_DATA, FORCE_DATA)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sophisticated-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulate import generate_fd\n",
    "DEBUG = False\n",
    "\n",
    "if DEBUG:\n",
    "    # multiple unfold cases\n",
    "    dist_unfold = 0.38\n",
    "    clslist = [[0.025], [0.015, 0.030], [0.01, 0.02, 0.03]]\n",
    "    for index, cls in enumerate(clslist):\n",
    "        curve_id = f'simulation_{index}'\n",
    "        fdcurves[curve_id] = \\\n",
    "            Curve(curve_id, *generate_fd(dist_unfold, cls, handle_estimates, protein_estimates))\n",
    "    # lost bead case\n",
    "    dist, force = generate_fd(0.38, [0.025], handle_estimates, protein_estimates)\n",
    "    force[1800:] = 0\n",
    "    fdcurves['sim_loss'] = Curve('sim_loss', dist, force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "temporal-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('/home/kris/proj/.data/tweez/split_curves.csv', 'r') as f:\n",
    "#with open('/home/kris/proj/.data/tweez/full.csv', 'r') as f:\n",
    " #   rows = [line.split('\\n')[0].split(',') for line in f]\n",
    "  #  print(rows[0][0])\n",
    "   # print(np.asarray([float(x) for x in rows[0][1:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-beach",
   "metadata": {},
   "source": [
    "### Above data gets ignored. Here we take a big trace and split it up into separate curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from autosplit import autosplit\n",
    "\n",
    "test_curves = autosplit('/home/kris/proj/.data/tweez/yhsp2.h5')\n",
    "\n",
    "fdcurves = {}  # clear fdcurces\n",
    "for key, curve in test_curves.items():\n",
    "    fdcurves[key] = Curve(key, *[curve[dname]\n",
    "                                for dname in ['full_dist', 'full_force',\n",
    "                                              'pull_dist', 'pull_force',\n",
    "                                              'rlx_dist', 'rlx_force']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-destruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import h5py\n",
    "#fdcurves = {}\n",
    "#with h5py.File('/home/kris/proj/.data/tweez/test.h5', 'r') as f:\n",
    "#    for key, cdict in f.items():\n",
    "#        fdcurves[key] = Curve(key, *[np.asarray(cdict[dname]) \n",
    "#                                     for dname in ['full_dist', 'full_force',\n",
    "#                                                   'pull_dist', 'pull_force',\n",
    "#                                                   'rlx_dist', 'rlx_force']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-memory",
   "metadata": {},
   "source": [
    "## Bead loss errors\n",
    "Data featuring force suddenly dropping to zero can dramatically slow down the event-finding and fitting steps, so we filter those out first. Other error-finding operations depend on the events found below and will happen later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "lost_bead_fdcurves = {}\n",
    "for curve_id, curve in fdcurves.items():\n",
    "    if curve.filter_bead_loss(handle_contour = handle_estimates['handles/Lc']['value']):\n",
    "        lost_bead_fdcurves[curve_id] = curve\n",
    "for curve_id in lost_bead_fdcurves.keys():\n",
    "    print(curve_id)\n",
    "    fdcurves.pop(curve_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-immigration",
   "metadata": {},
   "source": [
    "## Event-finding\n",
    "Here we try to find unfolding events, stationary/turning points and start-points. Unfolding events are recognized through a sharp dip in the force over time graph, and stationary/turning points by a sharp dip in the force's local variance. Start-points are determined by where the system's force first exceeds the ```STARTING_FORCE``` argument of the find_events function, which defaults to 0.\n",
    "\n",
    "Curves are then split into 'legs' of datapoints between events, and those legs are used to fit our different models. The ```SHOW_PLOTS``` option produces a plot for each curve, showing the different events and legs.\n",
    "\n",
    "Further (optional) arguments for the ```find_events``` function are:\n",
    "- ```CORDON```, which determines how many datapoints directly before and after an event we exclude from fitting. Defaults to 10,\n",
    "- ```FIT_ON_RETURN```, describes which part of the relaxation curve we mark for fitting. Defaults to an empty tuple, ignoring the relaxation curve. Valid input is a tuple like ```(WHEN, NUMBER)``` indicating how far after the return point to start fitting, and how many points to include.\n",
    "\n",
    "```SHOW_PLOTS``` determines whether to show plots highlighting the different events and legs for each curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_PLOTS = True\n",
    "%matplotlib inline\n",
    "\n",
    "for curve in fdcurves.values():\n",
    "    print(curve.identifier)\n",
    "    curve.find_events(DEBUG=True, handle_contour = handle_estimates['handles/Lc']['value'])\n",
    "    #print(curve.top)\n",
    "    if SHOW_PLOTS:\n",
    "        curve.plot_events()\n",
    "# plot force over time (or number of measurements)\n",
    "# green: fitted leg, orange: unfold event, red: return/stationary point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_unfolds = {}\n",
    "for curve_id, curve in fdcurves.items():\n",
    "    if not curve.unfolds.any(): #or not curve_id == 'split_curve9':\n",
    "        no_unfolds[curve_id] = curve  \n",
    "for curve_id in no_unfolds.keys():\n",
    "    print(curve_id)\n",
    "    fdcurves.pop(curve_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "for curve_id, curve in fdcurves.items():\n",
    "    print(curve_id)\n",
    "    #print(curve.top)\n",
    "    print(curve.unfolds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-avatar",
   "metadata": {},
   "source": [
    "## Error-finding\n",
    "##### Multiple tethers\n",
    "We filter multiple tethers comparison of the model as given in the estimates, with exaggerated parameters or data. The tests we perform include:\n",
    "- A doubled persistence length\n",
    "- A halved contour length\n",
    "- Halved force data\n",
    "- Doubled distance data\n",
    "\n",
    "These should in theory test the single-tether case against the double-tether case. For a greater number of tethers we assume a double-tether model would fit better than a single-tether one. We compare the resulting BICs to determine pass or fail. We also compute Bayes factors for relative likelihood comparisons, but those turn out rather extreme.\n",
    "\n",
    "Doubling the persistence length yields false positives and does not seem to be a good way to approximate a double tether. Some higher factor would probably work but for now we can just ignore that test.\n",
    "\n",
    "We may want to hide some of this code behind an import\n",
    "\n",
    "Halving contour length appears functionally identical to doubling the distance data, so we can do away with one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "original = handle_estimates\n",
    "double_Lp = deepcopy(original)\n",
    "half_Lc = deepcopy(original)\n",
    "double_Lp['handles/Lp']['value'] = original['handles/Lp']['value'] * 2\n",
    "half_Lc['handles/Lc']['value'] = original['handles/Lc']['value'] / 2\n",
    "\n",
    "test_estimates = {'original': original, 'double_Lp': double_Lp, \n",
    "                  'half_Lc': half_Lc, 'half_force': original,\n",
    "                  'double_dist': original}\n",
    "\n",
    "for curve in fdcurves.values():\n",
    "    print(curve.identifier)\n",
    "    curve.filter_tethers(build_handles_model(), test_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-somewhere",
   "metadata": {},
   "source": [
    "## Fitting\n",
    "\n",
    "Here we fit our models. We first define fits for each unfolding event, and proliferate a single fit of the DNA handles parameters to each of them. After that, we fit the remaining protein parameters with the legs we defined before. The third cell draws plots and shows parameter summaries for each curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-coating",
   "metadata": {},
   "outputs": [],
   "source": [
    "for curve in fdcurves.values():\n",
    "    print(curve.identifier, curve.legs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fdcurves.pop('split_curve4')\n",
    "#fdcurves.pop('split_curve9')\n",
    "#fdcurves.pop('split_curve5')\n",
    "\n",
    "for curve in fdcurves.values():\n",
    "    print(curve.identifier)\n",
    "    curve.initialize_fits(build_handles_model(),\n",
    "                         build_composite_model(),\n",
    "                         handle_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some fits may take a while. should finish though\n",
    "for curve_id, curve in fdcurves.items():\n",
    "    print(curve_id)\n",
    "    curve.fit_composites(protein_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for curve in fdcurves.values():\n",
    "    curve.plot_fits()\n",
    "    plt.show()\n",
    "    curve.print_fit_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-trouble",
   "metadata": {},
   "source": [
    "#### Computing unfolding forces\n",
    "We compute the force for each unfolding event by simulating the fitted model to the distance slightly before the unfolding event. This part is lacking an error estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "for curve in fdcurves.values():\n",
    "    curve.compute_unfold_forces(build_handles_model(), build_composite_model())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-strip",
   "metadata": {},
   "source": [
    "#### Result summary table\n",
    "Prints a summary table containing fitted contour length, persistence length and unfolding force for each unfolding domain. Could be prettier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score and/or rank by fit quality\n",
    "\n",
    "# make a format string for the column widths\n",
    "largest_id_len = max([len(id) for id in fdcurves.keys()])\n",
    "row_format = f'{{:<{largest_id_len + 1}}}| {{:<5}}| {{:<9}}| {{:<9}}| {{:<9}}| {{}}'\n",
    "\n",
    "print(row_format.format('Curve', 'fold', 'Lc (um)', 'Lp (nm)', 'Fu (pN)',\n",
    "                        'failed tests'))\n",
    "for curve_id, curve in fdcurves.items():\n",
    "    curve.print_result_rows(row_format)\n",
    "\n",
    "if lost_bead_fdcurves:\n",
    "    print('\\nBead losses for curves', [curve_id for curve_id in\n",
    "                                       lost_bead_fdcurves.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "from event_finding import moving_window_SLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = moving_window_SLR(fdcurves['split_curve9'].force_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-underwear",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "name": "CVbook.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
